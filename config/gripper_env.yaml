# Robot parameters
robot:
  model_path: assets/gripper/wsg50_one_motor_gripper_new.sdf
  max_translation: 0.03
  max_yaw_rotation: 0.15
  max_force: 100
  discrete: False
  step_size: 0.01 #For discrete action space
  yaw_step: 0.1
  num_actions_pad: 2
  # include_robot_height: True


# Experimental setup parameters
scene:
  scene_type: "OnTable" # OnTable scene is easier for the table clearing task
  # scene_type: OnFloor, OnTable, OnTray
  data_set: random_urdfs
  #data_set: random_urdfs, wooden_blocks, custom_urdfs

# Simulation parameters
simulation:
  visualize: False
  time_horizon: 150
  real_time: False
  gravity: False
  unit_time: False

sensor:
  camera_info: config/camera_info.yaml
  transform: config/camera_info.yaml
  encoder_dir: checkpoints/_final/encoder_simple_512
  encoder_type: full #[full, depth]
  visualize: False

  # Randomize camera parameters
  randomize:
    focal_length: 4
    optical_center: 2
    translation: 0.002
    rotation: 0.0349

# Custom shaped reward function parameters
reward:
  custom: True
  shaped: True
  terminal_reward: 10000.
  # lift_success: 1000.
  grasp_reward: 100.
  delta_z_scale: 1000.
  time_penalty: 200.
  table_clearing: False # False - picks only one item. With True the episode terminates after each objects are cleared. Table clearing works better with the curriculum parameter max object [1, 5] 

# Workspace curriculum parameters
curriculum:
  init_lambda: 0.
  n_steps: 4
  success_threshold: 0.7
  window_size: 500
  extent: [0.01, 0.1]
  robot_height: [0.15, 0.25]
  lift_dist: [0.015, 0.1]
  max_objects: [3, 5] # Change to [1, 5] when table clearing task is set
  min_objects: [1, 1]
  num_objects: 7
  # workspace: [0.2, 1]
  # work_height: [0.2, 1]

# Generate new initial states until at least on object is within the FOV
skip_empty_initial_state: True

# Use simplified problem formulation
simplified: False
# Depth + Actuator
depth_observation: False
# RGB + Depth + Actuator
full_observation: True
# Markov decision process parameters
discount_factor: 0.99
# normalize the input and rewards
normalize: True

# Policy algorithm
algorithm: sac

SAC:
  max_iters: 400
  batch_size: 64
  layers: [64, 64]
  buffer_size: 100000
  step_size: 0.0003
  tensorboard_logs: !!null
  total_timesteps: 2000000
  save_dir: sac2m_depth_obs