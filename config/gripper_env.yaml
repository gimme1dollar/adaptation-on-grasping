# Simulation parameters
simulation:
  real_time: False
  visualize: True
  mode: default
  time_horizon: 300
  max_episode: 10_000_000
  
  scene:
    scene_type: "OnTable" # OnTable scene is easier for the table clearing task
    #scene_type: "OnFloor"
    data_set: random_urdfs
    object_num: 15
    object_pos: [0.0, 0.0, 0.5]
    
# Robot parameters
robot:
  model_path: assets/gripper/wsg50_one_motor_gripper_new.sdf
  max_translation: 0.03
  max_yaw_rotation: 0.15
  max_force: 100
  discrete: False
  step_size: 0.01 #For discrete action space
  yaw_step: 0.1
  num_actions_pad: 2
  # include_robot_height: True

sensor:
  camera_info: 
    height: 64
    width: 64
    K: [69.76, 0.0, 32.19, 0.0, 77.25, 32.0, 0.0, 0.0, 1.0]
    near: 0.02
    far: 2.0
  transform: 
    translation: [0.0, 0.0573, 0.0451]
    rotation: [0.0, -0.1305, 0.9914, 0.0]
  visualize: True
  encode: False

  # Randomize camera parameters
  randomize:
    focal_length: 4
    optical_center: 2
    translation: 0.002
    rotation: 0.0349

  # Encoder parameters
  encoder:
    network: [
      {in_channel: 3, out_channel: 32, kernel_size: 3, stride: 1, padding: 1},
      {in_channel: 32, out_channel: 16, kernel_size: 3, stride: 1, padding: 1},
      {in_channel: 16, out_channel: 8, kernel_size: 3, stride: 1, padding: 1},
      {in_channel: 8, out_channel: 1, kernel_size: 1, stride: 1, padding: 0}
    ]
    encoding_dim: 100
    learning_rate: 0.0002
    batch_size: 128
    epochs: 120
    data_path: ~/ActionBranchingDQN/new_gripper_onfloor_imgs.pkl


# Custom shaped reward function parameters
reward:
  custom: True
  shaped: True
  terminal_reward: 10_000.
  lift_reward: 300. # lift target object
  grasp_reward: 100.
  detect_reward: 10. # detected from segmentation mask
  close_penalty: 10.
  out_penalty: 10.
  time_penalty: 20.
  table_clearing: False # False - picks only one item. With True the episode terminates after each objects are cleared. Table clearing works better with the curriculum parameter max object [1, 5] 

# Generate new initial states until at least on object is within the FOV
skip_empty_initial_state: True

# Use simplified problem formulation
simplified: False
# Depth + Actuator
depth_observation: True
# RGB + Depth + Actuator
full_observation: True
# Markov decision process parameters
discount_factor: 0.99
time_horizon: 300
# normalize the input and rewards
normalize: True

policy:
  algo_type: DDPG
  warm_start: 20

  DDPG:
    batch_size: 32
    tau: 0.1
    gamma: 0.99
    epsilon_start: 1
    epsilon_end: 0
    epsilon_decay: 200